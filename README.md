# llm-rag-inference
An end-to-end Retrieval-Augmented Generation (RAG) pipeline leveraging Azure OpenAI for LLM inference, Hugging Face embeddings for vectorization, and ChromaDB for efficient knowledge retrieval.
